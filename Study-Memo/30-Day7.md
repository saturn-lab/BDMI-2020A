## WEEK7 学习小结

程宇笑 自84 2018010888

---

## Numpy 简介

numpy 最显著的特点是 N 维数组 ndarray


```python
import numpy as np
a = np.array([[1,2],[3,4]], dtype=float)
print(type(a))
print(a)
print(np.empty([2,3]))
```

    <class 'numpy.ndarray'>
    [[1. 2.]
     [3. 4.]]
    [[0. 1. 2.]
     [3. 4. 5.]]
    

#### Numpy 广播和形状变换

对一个数组的操作可以广播到每一个元素。

数组还可以进行形状变换，但前提是新形状兼容原形状。

使用 transpose 函数可以进行转置，


```python
a = np.sin(a)
print(a)
b = np.reshape(a, [1, 4])
print(b)
print(np.transpose(a))
print(a.transpose(1, 0))
```

    [[ 0.84147098  0.90929743]
     [ 0.14112001 -0.7568025 ]]
    [[ 0.84147098  0.90929743  0.14112001 -0.7568025 ]]
    [[ 0.84147098  0.14112001]
     [ 0.90929743 -0.7568025 ]]
    [[ 0.84147098  0.14112001]
     [ 0.90929743 -0.7568025 ]]
    

除此之外，Numpy 拥有大量的数学函数、统计、排序、线性代数运算功能。

还可以将结果保存为文件：


```python
np.save("a.npy", a)
print(np.load("a.npy"))
```

    [[ 0.84147098  0.90929743]
     [ 0.14112001 -0.7568025 ]]
    

## 深度学习入门

### 人工神经元

人工神经元的基本模式是多个输入进行**加权线性叠加**（可能还会叠加一个常数），通过一个**非线性**激活函数进行输出。

人工神经元的激活函数有 sigmoid, ranh, ReLU 函数。


```python
import matplotlib.pyplot as plt

x = np.linspace(-3, 3, 100)
relu = lambda x: np.maximum(x, 0)
plt.plot(x, relu(x))

softplus = lambda x: np.log(1.0 + np.exp(x))
plt.plot(x, softplus(x))

sigmoid = lambda x: 1 / (1 + np.exp(-x))
plt.plot(x,sigmoid(x), color='red', lw=2)
plt.show()
```


### 人工神经元模拟布尔电路

通过线性叠加和激活函数，可以模拟出与、或、非运算。但并不能直接模拟出**异或运算**（XOR 是非线性分类问题），需要多个神经元叠加。

大量的人工神经元可以构成人工神经网络，可以解决简单分类问题。

### 人工神经网络

不同神经元的连接关系可以分为**前馈网络**，**反馈网络**，**记忆网络**。

前馈网络的输出仅传向下一层，卷积神经网络（CNN）就是一种前馈网络。卷积网络包括**卷积层**（卷积核是多维数组，在学习中改变），**下采样层（池化层）**（进行最大值或最小值的抽样，输入下一层），**随机丢弃层**（每一次学习中，禁用随机的一些神经元，这对于防止过拟合很有用）。

理论上讲，前馈网络可以拟合出任何函数。

反馈网络当前时间的输出可以作为下一时间的输入，循环神经网络（RNN）是一种反馈网络。理论上讲只要给出正确的权重，循环神经网络能完成任何运算。

### 损失函数

损失函数是**当前输出和预期输出的度量**，例如交叉熵、平方差。通常回归问题采用均方差，分类问题选择交叉熵。

深度学习的目标即是调整参数使得**损失函数最小**，常用的办法是梯度下降法，使用反向链式法则求微分（反向传播）。下降的速率称为**学习率**，学习率太大容易引起震荡，学习率太小则收敛太慢。

随机梯度下降算法（**SGD**）是最常用的权重调节方法，使用全部数据做一个训练批次，优点是收敛性有保证，缺点是参数更新前需要遍历整个数据集。因此通常设置小批量训练（mini-batch）。

## Tensorflow 入门

### 数据的张量（Tensor）表示

张量（Tensor）在形式上与向量（Vector）差别不大，可以表示任意维度的数据。

### Tensorflow 线性回归

```python
# Optimization process. 
def run_optimization():
    # Wrap computation inside a GradientTape for automatic differentiation.
    with tf.GradientTape() as g:
        pred = linear_regression(X) # 使用当前参数进行预测
        loss = mean_square(pred, Y) # mean_square 预先定义，计算出当前预测与实际值的“差异”

    # Compute gradients.
    gradients = g.gradient(loss, [W, b])
    
    # optimizer = tf.optimizers.SGD(learning_rate)
    # Update W and b following gradients.
    optimizer.apply_gradients(zip(gradients, [W, b])) # zip: 将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表。
```
